{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "217a9598",
   "metadata": {},
   "source": [
    "# Brief Review of Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457bd7e1",
   "metadata": {},
   "source": [
    "What makes reinforcement learning different from other machine learning paradigms?\n",
    "- There is no supervisor, only a reward signal\n",
    "- Feedback is delayed, not instantaneous\n",
    "- Time really matters (sequential, non i.i.d data)\n",
    "- Agent‚Äôs actions affect the subsequent data it receives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7a81ca",
   "metadata": {},
   "source": [
    "## Rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d14c1c",
   "metadata": {},
   "source": [
    "A reward $R_{t}$ is a scalar feedback signal\n",
    "\n",
    "Indicates how well agent is doing at step $t$\n",
    "\n",
    "The agent‚Äôs job is to maximise cumulative reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc62eb6",
   "metadata": {},
   "source": [
    "Reinforcement learning is based on the **reward hypothesis**\n",
    "\n",
    "*: All goals can be described by the maximisation of expected cumulative reward*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0eb0ae",
   "metadata": {},
   "source": [
    "## Sequential Decision Making"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58e3c93",
   "metadata": {},
   "source": [
    "- It means making a series of decisions or taking a sequence of actions over time to achieve a long-term goal\n",
    "\n",
    "- Goal: select actions to maximise total future reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a53ba6c",
   "metadata": {},
   "source": [
    "*E.g*\n",
    "\n",
    "Let's say you're playing a game where you're trying to reach a treasure chest at the end of a maze. You start off in a room with multiple doors, and you have to choose which door to go through to get to the next room. Each door leads to a different room, and some rooms have traps that will cause you to lose a life.\n",
    "\n",
    "In this scenario, sequential decision making means that you have to make a series of decisions, one after the other, in order to reach your goal of getting to the treasure chest. Each decision you make affects your chances of success, because some rooms have traps and some don't, and you won't know which is which until you enter them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90164a3f",
   "metadata": {},
   "source": [
    "- Actions may have long term consequences\n",
    "\n",
    ": Actions having long-term consequences means that the decision you make at each stage of the game can have an impact on the rest of the game."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761078da",
   "metadata": {},
   "source": [
    "- Reward may be delayed\n",
    "\n",
    ": The agent may not receive a reward immediately after taking an action. Instead, the reward may be delayed and only received after some time has passed or after a sequence of actions has been taken."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4461701",
   "metadata": {},
   "source": [
    "- It may be better to sacrifice immediate reward to gain more long-term reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab4e2a8",
   "metadata": {},
   "source": [
    "## Agent and Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a33832",
   "metadata": {},
   "source": [
    "- Agent\n",
    "\n",
    "An agent is an entity that learns to perform actions in an environment to maximize a cumulative reward signal. The agent interacts with the environment by taking actions and observing the resulting state and reward, and uses this information to improve its decision-making process over time. The goal of the agent is to learn a policy that maps states to actions in a way that maximizes the expected cumulative reward."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af7a295",
   "metadata": {},
   "source": [
    "- Environment\n",
    "\n",
    "The environment is the problem that the agent is trying to solve, and the agent's goal is to learn how to interact with the environment in a way that maximizes its rewards."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d837f7",
   "metadata": {},
   "source": [
    "- obeservation : $O_{t}$\n",
    "\n",
    "Observation refers to the current state of the environment as perceived by the agent. Observations can include information about the agent's position, velocity, sensory input, and any other relevant features of the environment that are necessary for the agent to make decisions and take actions. The agent uses its observations to learn about the environment and determine the best actions to take in order to achieve its goals. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb57600c",
   "metadata": {},
   "source": [
    "- action: $A_{t}$\n",
    "\n",
    "An action refers to the decision made by an agent at a particular time step in response to the observation it receives from the environment. It is the agent's way of influencing the environment in order to achieve a certain goal or maximize its cumulative reward. The action can take various forms depending on the specific problem, such as moving a robot, playing a move in a game, or selecting an advertisement to display to a user."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7276500",
   "metadata": {},
   "source": [
    "$t$ increments at env. step\n",
    "\n",
    "At each step $t$ the agent:\n",
    "- Executes $A_{t}$\n",
    "- Receives $O_{t}$\n",
    "- Receives $R_{t}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e18ca3",
   "metadata": {},
   "source": [
    "The environment:\n",
    "- Recevies $A_{t}$\n",
    "- Emits $O_{t+1}$\n",
    "- Emits $R_{t+1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e800ff1d",
   "metadata": {},
   "source": [
    "<img src=\"RL1.png\" alt=\"RL1\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2edda4",
   "metadata": {},
   "source": [
    "## History and State"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6744d303",
   "metadata": {},
   "source": [
    "- History: $H_{t}$\n",
    "\n",
    ": The history is the sequence of observations, actions and rewards\n",
    "\n",
    "$H_{t} = O_{1},R_{1},A_{1},...,A_{t‚àí1},O_{t},R_{t}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ee9cdd",
   "metadata": {},
   "source": [
    "- State: $S_{t} = f(H_{t})$\n",
    "\n",
    ": The information used to determine what happens next"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a17885",
   "metadata": {},
   "source": [
    "## Environment state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17b1f8b",
   "metadata": {},
   "source": [
    "The environment state $S_{t}^{e}$ is the environment's private representation\n",
    "\n",
    "The environment's internal representation of its current state. It is private because the agent does not have direct access to it and must infer it based on its own observations and actions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befba87f",
   "metadata": {},
   "source": [
    "The environment state is not usually visible to the agent\n",
    "\n",
    "*Q. I think agent use 'environment', but the environment state is not usually visible to the agent. Then why?*\n",
    "\n",
    "- The agent's interaction with the environment (through observations and actions) can provide information that allows the agent to make inferences about the environment state."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbffcff3",
   "metadata": {},
   "source": [
    "Even if $S_{t}^{e}$ is visible, it may contain irrelevant information\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74fa946b",
   "metadata": {},
   "source": [
    "## Agent state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5608e78f",
   "metadata": {},
   "source": [
    "The agent state $S_{t}^{a}$ is the agent's internal representation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18765cc",
   "metadata": {},
   "source": [
    "- Whatever information the agent uses to pick the next action\n",
    "- It is the information used by RL algos\n",
    "- It can be any function of history : the agent can use any combination of past observations and actions in order to determine its next action.\n",
    "\n",
    "\n",
    "$S_{t}^{a} = f(H_{t})$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434b95e9",
   "metadata": {},
   "source": [
    "## Information state(Markov state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9eaae3",
   "metadata": {},
   "source": [
    "An information state contains all useful information form the history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf2ee77",
   "metadata": {},
   "source": [
    "- Definition\n",
    "\n",
    "A state $S_{t}$ is Markov if and only if $\\mathbb{P}[S_{t+1}|S_{t}] = \\mathbb{P}[S_{t+1}| S_{1}, ... , S_{t}]$\n",
    "\n",
    "The statement means that a state at time t is Markovian if and only if the probability of transitioning to the next state at time t+1, given the current state at time t, is equal to the probability of transitioning to the next state at time t+1, given all the previous states from time 1 up to time t. In other words, the current state contains all the relevant information necessary to predict the future, and there is no additional information from the past that is needed to make accurate predictions.\n",
    "\n",
    "So if we know everything, then we can say it's *'Markovian'*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e0c9f4",
   "metadata": {},
   "source": [
    "If the probability of transitioning to the next state at time t+1, given the current state at time t, is not equal to the probability of transitioning to the next state at time t+1, given all the previous states from time 1 up to time t, then the state is *not Markovian.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0391554",
   "metadata": {},
   "source": [
    "The future is independent of the past given the present\n",
    "\n",
    ": the future state depends only on the present state and not on the history of states.\n",
    "\n",
    "$H_{1:t} -> S_{t} -> H_{t+1:\\infty}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e19bd4d",
   "metadata": {},
   "source": [
    "Once the state is known, the history may be thrown away"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84613964",
   "metadata": {},
   "source": [
    "The environment state $S_{t}^{e}$ is Markov\n",
    "\n",
    ": Because it satisfies the Markov property. Specifically, the environment state at time t contains all the information necessary to predict the future evolution of the environment, given the current observation and action of the agent. In other words, the environment state at time t summarizes all relevant information from the past that is necessary to make accurate predictions about the future. This means that the environment state satisfies the Markov property, which is a key assumption in many reinforcement learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9aa0e08",
   "metadata": {},
   "source": [
    "E.g \n",
    "\n",
    "Suppose you have a robot vacuum cleaner that can move around a room and clean up dirt. The state of the environment at time t could be the positions of the robot and the dirt, as well as other relevant information such as the location of obstacles in the room. The robot's sensors can detect the current position and any obstacles or dirt nearby, and its actions are to move in a certain direction or to clean the dirt at its current position.\n",
    "\n",
    "The environment state is Markov in this case because the future state of the environment (i.e. the positions of the robot and dirt at time t+1) only depends on the current state of the environment (i.e. the positions of the robot and dirt at time t) and the action taken by the robot at time t. There is no additional information from past states that is necessary to predict the future state."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1300807",
   "metadata": {},
   "source": [
    "The history $H_{t}$ is Markov\n",
    "\n",
    ": Because it contains all the relevant information needed to predict the future. Specifically, the history Ht includes the sequence of observations and actions up to time t, which fully captures the state of the environment at time t. Given Ht, the probability of transitioning to the next state St+1 depends only on St and At, the current state and action, and not on any previous states or actions. This satisfies the Markov property, which states that the future is independent of the past given the present."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6202df84",
   "metadata": {},
   "source": [
    "E.g\n",
    "\n",
    "Suppose a robot is navigating a room and has sensors that can detect walls and obstacles, as well as motors to control its movement. The robot's history Ht includes all of the sensor readings and motor commands up to time t. If the robot's sensors and motors are designed in such a way that the current sensor readings are sufficient to predict the next sensor readings and necessary motor commands, then the robot's history Ht is Markov. This means that the current state of the robot's sensor and motor system contains all of the relevant information needed to predict the future."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7235da51",
   "metadata": {},
   "source": [
    "## Fully Observable Environments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78a7f85",
   "metadata": {},
   "source": [
    "- Full observability\n",
    "\n",
    ": agent directly observes environment state\n",
    "\n",
    "$O_{t} = S_{t}^{a} = S_{t}^{e}$ (Agent state = environment state = information state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27379e35",
   "metadata": {},
   "source": [
    "Fomally, this is a Markov decision process (MDP) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc151e1d",
   "metadata": {},
   "source": [
    "## Partial Observable Environments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49c2bf1",
   "metadata": {},
   "source": [
    "- Partial observability: agent indirectly observes environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3bf4e51",
   "metadata": {},
   "source": [
    "E.g\n",
    "- A robot with camera vision isn't told its absolute location\n",
    "- A trading agent only observes current prices\n",
    "- A poker playing agent only observes public cards\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936ff964",
   "metadata": {},
   "source": [
    "Now agent state != environment state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e4427d",
   "metadata": {},
   "source": [
    "Formally this is a partially observable Markov decision process (POMDP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38cefee",
   "metadata": {},
   "source": [
    "Agent must construct its own state representation $S_{t}^{a}$, e.g.\n",
    "- complete history: $S_{t}^{a} = H_{t}$\n",
    "- Beliefs of environment state : $S_{t}^{a} = (\\mathbb{P}[S_{t}^{e} = s^{1}, ... , \\mathbb{P}[S_{t}^{e} = s^{n})]$\n",
    "- RNN: $S_{t}^{a} = \\sigma(S_{t-1}^{a}W_{s} + O_{t}W_{o})$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428987a2",
   "metadata": {},
   "source": [
    "## Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85724b0c",
   "metadata": {},
   "source": [
    ": A policy is the agent's behaviour"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33264079",
   "metadata": {},
   "source": [
    "- Deterministic policy: $a = \\pi(s)$\n",
    "- Stochastic policy: $\\pi(a|s) = \\mathbb{P}[A_{t} = a|S_{t} = s]$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a2b77b",
   "metadata": {},
   "source": [
    "![RL2](RL2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351db182",
   "metadata": {},
   "source": [
    "## Value Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6268d2c1",
   "metadata": {},
   "source": [
    ": Value function is a prediction of future reward, used to evaluate the goodness / badness of states"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92112e88",
   "metadata": {},
   "source": [
    "- $v_{\\pi}(s) = \\mathbb{E}_{\\pi}[R_{t+1} + \\gamma R_{t+2} + \\gamma^{2} R_{t+3} + ... |S_{t} = s]$\n",
    "\n",
    "$ùëâ_{ùúã}(ùë†)$: the state-value function for policy ùúã at state ùë†, which represents the expected return (i.e., the sum of discounted rewards) when starting from state ùë† and following policy ùúã thereafter.\n",
    "\n",
    "$ùê∏_{ùúã}[ùëÖ_{ùë°+1}+ùõæùëÖ_{ùë°+2}+ùõæ^{2}ùëÖ_{ùë°+3}+...|ùëÜ_{ùë°}=ùë†]$: the expected return for the current state ùë†, which is the sum of the expected immediate reward $ùëÖ_{ùë°+1}$ and the expected return for the next state $ùëÜ_{ùë°+1}$ under the same policy ùúã, which is discounted by a factor of ùõæ (i.e., the discount rate). This process continues until the end of the episode or termination of the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e17d1d",
   "metadata": {},
   "source": [
    "![RL3](RL3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7efb1b",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da48a56",
   "metadata": {},
   "source": [
    ": A model predicts what the environment will do next"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac23029",
   "metadata": {},
   "source": [
    "- $P$ predicts the next state\n",
    "- $R$ predicts the next (immediate) reward, e.g.\n",
    "\n",
    "$P_{ss'}^{a} = \\mathbb{P}[S_{t+1} = s' | S_{t} = s, A_{t} = a]$\n",
    "\n",
    "$R_{s}^{a} = \\mathbb{E}[R_{t+1} | S_{t} = s, A_{t} = a]$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73290aa1",
   "metadata": {},
   "source": [
    "![RL4](RL4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3004996",
   "metadata": {},
   "source": [
    "## Categorizing RL agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4b7a22",
   "metadata": {},
   "source": [
    "- Value Based: No policy, Value Function\n",
    "- Policy Based: Policy, No Value Function\n",
    "- Actor Critic: Policy, Value Function\n",
    "- Model Free: Policy and/ or Value Function, No model\n",
    "- Model Based: Policy and/ or Value Function, Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03478b40",
   "metadata": {},
   "source": [
    "![RL5](RL5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f96e3b8",
   "metadata": {},
   "source": [
    "## Learning and Planning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e326635",
   "metadata": {},
   "source": [
    ": Two fundamental problems in sequential decision making"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f69c3da",
   "metadata": {},
   "source": [
    "Reinforcement Learning:\n",
    "- The environment is initially unknown\n",
    "- The agent interacts with the environment\n",
    "- The agent improves its policy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec66b92",
   "metadata": {},
   "source": [
    "Planning:\n",
    "- A model of the environment is known\n",
    "- The agent performs computations with its model (without any external interaction)\n",
    "- The agent improves its policy\n",
    "- a.k.a deliberation, reasoning, introspection, pondering, thought, search\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28cf8831",
   "metadata": {},
   "source": [
    "The agent does not have any prior knowledge about the environment it is interacting with. The agent learns about the environment by taking actions and receiving feedback in the form of rewards or punishments. Based on this feedback, the agent updates its policy to maximize the total reward it can receive from the environment. In simple terms, the agent learns by trial and error.\n",
    "\n",
    "On the other hand, in planning, the agent already has a model of the environment, which means it knows how the environment works and what the outcomes of different actions will be. The agent can use this model to perform computations and simulations without any external interaction with the environment. Based on the results of these simulations, the agent can improve its policy to achieve the desired goals.\n",
    "\n",
    "To summarize, the key difference between reinforcement learning and planning is that in reinforcement learning, the agent learns from its interaction with the environment, while in planning, the agent uses its knowledge of the environment to make decisions without any interaction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ecee3d1",
   "metadata": {},
   "source": [
    "## Exploration and Exploitation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71f76f1",
   "metadata": {},
   "source": [
    "- RL is like trial-and-error learning\n",
    "- The agent should discover a good policy\n",
    "- From its experiences of the environment\n",
    "- Without losing too much reward along the way"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a07e8b",
   "metadata": {},
   "source": [
    "*Exploration* finds more information about the environment\n",
    "\n",
    "*Exploitation* exploits known information to maximise reward\n",
    "\n",
    "**It is usually important to explore as well as exploit**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d838b5ea",
   "metadata": {},
   "source": [
    "E.g\n",
    "\n",
    "1. Restaurant Selection:\n",
    "- Exploitation: Go to your farvorite restaurant\n",
    "- Exploration: Try a new restaurant\n",
    "\n",
    "2. Online Banner Advertisements:\n",
    "- Exploitation: Show the most successful advert\n",
    "- Exploration: Show a different advert\n",
    "\n",
    "3. Oil Drilling\n",
    "- Exploitation: Drill at the best known location\n",
    "- Exploration: Drill at a new location\n",
    "\n",
    "4. Game Playing\n",
    "- Exploitation: Play the move you believe is best\n",
    "- Exploration: Play an experimental move\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83131413",
   "metadata": {},
   "source": [
    "## Prediction and Control"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a1223e",
   "metadata": {},
   "source": [
    "- Prediction: evaluate the future (Given a policy)\n",
    "- Control: optimise the future (Find the best policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11560332",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
